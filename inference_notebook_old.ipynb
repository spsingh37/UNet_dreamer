{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "from inference import predict\n",
    "from transformations import normalize_01, re_normalize\n",
    "from unet import UNet\n",
    "import cv2\n",
    "# root directory\n",
    "# root = pathlib.Path('/mnt/c/Users/crsoc/Documents/roahm/camera_preprocessing/UNet/dataset_large/val')\n",
    "# root = pathlib.Path('/mnt/c/Users/crsoc/Documents/roahm/camera_preprocessing/cleaned_images/imgs_ftg_1')\n",
    "# root = pathlib.Path(r'C:\\Users\\Lenovo\\Downloads\\Summer 2023\\programming\\Seg-camera training\\UNet\\dataset_large\\val')\n",
    "# root = pathlib.Path(r'E:\\dataset_large\\carpet\\val')\n",
    "# root = pathlib.Path(r'C:\\Users\\Lenovo\\Downloads\\testing_target')\n",
    "root = pathlib.Path(r'C:\\Users\\Lenovo\\Downloads\\Summer 2023\\programming\\Seg-camera training\\UNet\\dataset_large\\val')\n",
    "\n",
    "def get_filenames_of_path(path: pathlib.Path, ext: str = \"*\"):\n",
    "    \"\"\"Returns a list of files in a directory/path. Uses pathlib.\"\"\"\n",
    "    filenames = [file for file in path.glob(ext) if file.is_file()]\n",
    "    return filenames\n",
    "\n",
    "\n",
    "# input and target files\n",
    "images_names = get_filenames_of_path(root / \"images\")\n",
    "# images_names = get_filenames_of_path(root)\n",
    "targets_names = get_filenames_of_path(root / \"targets\")\n",
    "\n",
    "# read images and store them in memory\n",
    "images = [imread(str(img_name)) for img_name in images_names]\n",
    "targets = [imread(tar_name) for tar_name in targets_names]\n",
    "\n",
    "# Resize images and targets\n",
    "images_res = [resize(img, (64,64, 3)) for img in images]\n",
    "resize_kwargs = {\"order\": 0, \"anti_aliasing\": False, \"preserve_range\": True}\n",
    "targets_res = [resize(tar, (64, 64), **resize_kwargs) for tar in targets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# model\n",
    "model = UNet(\n",
    "    in_channels=3,\n",
    "    out_channels=4,\n",
    "    n_blocks=4,\n",
    "    start_filters=32,\n",
    "    activation=\"relu\",\n",
    "    normalization=\"batch\",\n",
    "    conv_mode=\"same\",\n",
    "    dim=2,\n",
    ").to(device)\n",
    "# model_name = \"checkpoints/experiment_lr_on_plateau_dataset_large_color_aug_1692224712.7671678/epoch_78.pth\"\n",
    "model_name = \"checkpoints\\experiment_lr_on_plateau_dataset_large_color_aug_1717479001.1984859/epoch_200.pth\"\n",
    "model_weights = torch.load(pathlib.Path.cwd() / model_name, map_location=device)\n",
    "\n",
    "model.load_state_dict(model_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# preprocess function\n",
    "def preprocess(img: np.ndarray):\n",
    "    img = np.moveaxis(img, -1, 0)  # from [H, W, C] to [C, H, W]\n",
    "    img = normalize_01(img)  # linear scaling to range [0-1]\n",
    "    img = np.expand_dims(img, axis=0)  # add batch dimension [B, C, H, W]\n",
    "    img = img.astype(np.float32)  # typecasting to float32\n",
    "    return img\n",
    "\n",
    "\n",
    "# postprocess function\n",
    "def postprocess(img: torch.tensor):\n",
    "    img = torch.argmax(img, dim=1)  # perform argmax to generate 1 channel\n",
    "    img = img.cpu().numpy()  # send to cpu and transform to numpy.ndarray\n",
    "    img = np.squeeze(img) # remove batch dim and channel dim -> [H, W]\n",
    "    img = re_normalize(img)  # scale it to the range [0-255]\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# predict the segmentation maps\n",
    "output = [predict(img, model, preprocess, postprocess, device) for img in images_res]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m dice \u001b[38;5;241m=\u001b[39m dice_loss(y_true, y_pred)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mdice)\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mdice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39margmin(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mdice)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mdice))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# print(targets_res[0].shape)\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1e-5):\n",
    "    intersection = torch.sum(y_true * y_pred, dim=(1,2,3))\n",
    "    sum_of_squares_pred = torch.sum(torch.square(y_pred), dim=(1,2,3))\n",
    "    sum_of_squares_true = torch.sum(torch.square(y_true), dim=(1,2,3))\n",
    "    dice = 1 - (2 * intersection + smooth) / (sum_of_squares_pred + sum_of_squares_true + smooth)\n",
    "    return dice\n",
    "\n",
    "y_true = torch.zeros(len(images),4,64,64)\n",
    "y_pred = torch.zeros(len(images),4,64,64)\n",
    "\n",
    "# print(y_true.shape)\n",
    "for i in range(len(images)):\n",
    "    temp = torch.zeros(64,64)\n",
    "    temp[targets_res[i]==0] = 1\n",
    "    y_true[i,0,:,:] = temp \n",
    "\n",
    "    temp = torch.zeros(64,64)\n",
    "    temp[targets_res[i]==1] = 1\n",
    "    y_true[i,1,:,:] = temp\n",
    "\n",
    "    temp = torch.zeros(64,64)\n",
    "    temp[targets_res[i]==2] = 1\n",
    "    y_true[i,2,:,:] = temp\n",
    "\n",
    "    temp = torch.zeros(64,64)\n",
    "    temp[targets_res[i]==3] = 1\n",
    "    y_true[i,3,:,:] = temp\n",
    "    \n",
    "    # print(output[i])\n",
    "    temp1 = torch.zeros(64,64)\n",
    "    temp1[output[i]==0] = 1\n",
    "    y_pred[i,0,:,:] = temp1 \n",
    "\n",
    "    temp1 = torch.zeros(64,64)\n",
    "    temp1[output[i]==1] = 1\n",
    "    y_pred[i,1,:,:] = temp1\n",
    "\n",
    "    temp1 = torch.zeros(64,64)\n",
    "    temp1[output[i]==2] = 1\n",
    "    y_pred[i,2,:,:] = temp1\n",
    "\n",
    "    temp1 = torch.zeros(64,64)\n",
    "    temp1[output[i]==3] = 1\n",
    "    y_pred[i,3,:,:] = temp1\n",
    "\n",
    "dice = dice_loss(y_true, y_pred)\n",
    "\n",
    "print(1-dice)\n",
    "print(torch.min(1-dice))\n",
    "print(torch.argmin(1-dice)+1)\n",
    "print(torch.mean(1-dice))\n",
    "\n",
    "for i in range(len(images)):  \n",
    "    if i>=0:\n",
    "        print(i+1)\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(images_res[i])\n",
    "        plt.title(\"Image\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(132)\n",
    "        temp1 = np.zeros((64, 64, 3), dtype=np.uint8)\n",
    "        temp1[targets_res[i]==0] = [30, 119, 180]\n",
    "        temp1[targets_res[i]==1] = [140, 86, 75]\n",
    "        temp1[targets_res[i]==2] = [22, 190, 207]\n",
    "        temp1[targets_res[i]==3] = [220, 19, 20]\n",
    "        # plt.imshow(output[i]*127, cmap='gray')\n",
    "        plt.imshow(temp1)\n",
    "        plt.title(\"Target\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(133)\n",
    "        temp = np.zeros((64, 64, 3), dtype=np.uint8)\n",
    "        temp[output[i]==0] = [30, 119, 180]\n",
    "        temp[output[i]==1] = [140, 86, 75]\n",
    "        temp[output[i]==2] = [22, 190, 207]\n",
    "        temp[output[i]==3] = [220, 19, 20]\n",
    "        plt.imshow(temp)\n",
    "        plt.title(\"Prediction\")\n",
    "        plt.axis('off')    \n",
    "        \n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
